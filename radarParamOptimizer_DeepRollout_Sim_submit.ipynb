{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c3b651",
   "metadata": {},
   "source": [
    "Real Time Radar Parameter Optimizer\n",
    "===================================\n",
    "\n",
    "Carson Anderson & Calvin Henggeler  \n",
    "ASEN 5264 Decision Making Under Uncertainty - Spring 2025  \n",
    "Semester Project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf58a16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Documents/DMU_Project_Local/RadarEnv`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"$(homedir())/Documents/DMU_Project_Local/RadarEnv\")\n",
    "\n",
    "# # # pin POMDPs back to the 0.9.x series (which still exports initialstate_distribution)\n",
    "# Pkg.add(Pkg.PackageSpec(name=\"POMDPs\",       version=\"0.9.6\"))\n",
    "\n",
    "# # install the helper‐packages at the last releases built against POMDPs 0.9.x\n",
    "# Pkg.add(Pkg.PackageSpec(name=\"POMDPModelTools\", version=\"0.3.13\"))\n",
    "# Pkg.add(Pkg.PackageSpec(name=\"BeliefUpdaters\",   version=\"0.2.2\"))\n",
    "# Pkg.add(Pkg.PackageSpec(name=\"POMDPPolicies\",    version=\"0.4.2\"))\n",
    "\n",
    "# # now rebuild everything\n",
    "# Pkg.precompile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f00e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using POMDPTools: DiscreteUpdater, ImplicitDistribution, RolloutSimulator, EpsGreedyPolicy\n",
    "using QuickPOMDPs: QuickPOMDP\n",
    "using POMDPTesting: has_consistent_distributions\n",
    "using QMDP: QMDPSolver\n",
    "using Plots\n",
    "using Statistics: std\n",
    "using POMDPPolicies: alphavectors, FunctionPolicy\n",
    "using Random, Distributions\n",
    "using ParticleFilters\n",
    "using BasicPOMCP\n",
    "using POMDPModelTools  # for ParticleCollection\n",
    "include(\"radarFunctions.jl\")\n",
    "include(\"radarSimulator.jl\")\n",
    "\n",
    "using Random\n",
    "\n",
    "struct RadarState\n",
    "    x::Float64\n",
    "    y::Float64\n",
    "end\n",
    "\n",
    "struct RadarObservation\n",
    "    x_belief::Float64\n",
    "    y_belief::Float64\n",
    "end\n",
    "\n",
    "# How to make a random RadarState (normal rand)\n",
    "Random.rand(rng::AbstractRNG, ::Type{RadarState}) = RadarState(rand(rng)*20000 - 10000, rand(rng)*20000 - 10000)\n",
    "\n",
    "# Tell Julia how to sample from RadarState\n",
    "Random.Sampler(::Type{RadarState}) = Random.SamplerTrivial(RadarState)\n",
    "\n",
    "# Tell Julia how to sample when Random.Sampler is used (POMDPTools uses this!)\n",
    "Base.rand(rng::AbstractRNG, sampler::Random.SamplerTrivial{RadarState, Any}) = RadarState(rand(rng)*20000 - 10000, rand(rng)*20000 - 10000)\n",
    "\n",
    "# How to randomly generate a RadarObservation\n",
    "Random.rand(rng::AbstractRNG, ::Type{RadarObservation}) = RadarObservation(randn(rng)*10000, randn(rng)*10000)\n",
    "\n",
    "# Tell Julia how to sample\n",
    "Random.Sampler(::Type{RadarObservation}) = Random.SamplerTrivial(RadarObservation)\n",
    "\n",
    "# Define the random sampling behavior when used through SamplerTrivial\n",
    "Base.rand(rng::AbstractRNG, sampler::Random.SamplerTrivial{RadarObservation, Any}) = RadarObservation(randn(rng)*10000, randn(rng)*10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7902d045",
   "metadata": {},
   "source": [
    "## 1. Create Smart Radar POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df31d1",
   "metadata": {},
   "source": [
    "### Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f13f77a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Receiver}:\n",
       " Receiver([20.04008016031912, 20.04008016031912], Float64[], 3.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================\n",
    "# --- ENVIRONMENT ---\n",
    "# ===================\n",
    "\n",
    "# Environment Grid\n",
    "x_max_size = 10000.0\n",
    "y_max_size = 10000.0    \n",
    "divisions  = 500\n",
    "x = collect(LinRange(-x_max_size, x_max_size, divisions))\n",
    "y = collect(LinRange(-y_max_size, y_max_size, divisions))\n",
    "global env = RadarEnvironment(x, y)\n",
    "\n",
    "# Receiver (same as transmitter)\n",
    "pos_rx = SVector(0.0, 0.0)\n",
    "snapped_pos_rx = snap_to_grid(env.grid_x, pos_rx)\n",
    "rx = Receiver(snapped_pos_rx, Float64[], 3.0)\n",
    "add_receiver!(env, rx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c81b099",
   "metadata": {},
   "source": [
    "### Radar POMDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153c0313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuickPOMDP{Base.UUID(\"5ed31efa-ea1f-4e8f-9d88-1c9876e7b50b\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#30#38\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#27#35\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#26#34\", reward::var\"#28#36\", initialstate::ImplicitDistribution{var\"#29#37\", Tuple{}}}}((stateindex = Dict{RadarState, Int64}(RadarState(-4500.0, 2000.0) => 996, RadarState(8000.0, -7000.0) => 283, RadarState(9500.0, 6500.0) => 1393, RadarState(-2000.0, -7000.0) => 263, RadarState(0.0, -6500.0) => 308, RadarState(-3000.0, -2500.0) => 630, RadarState(1000.0, -8500.0) => 146, RadarState(-3500.0, 3500.0) => 1121, RadarState(-500.0, 10000.0) => 1660, RadarState(-4000.0, 1000.0) => 915…), isterminal = var\"#30#38\"(), states = RadarState[RadarState(-10000.0, -10000.0) RadarState(-10000.0, -9500.0) … RadarState(-10000.0, 9500.0) RadarState(-10000.0, 10000.0); RadarState(-9500.0, -10000.0) RadarState(-9500.0, -9500.0) … RadarState(-9500.0, 9500.0) RadarState(-9500.0, 10000.0); … ; RadarState(9500.0, -10000.0) RadarState(9500.0, -9500.0) … RadarState(9500.0, 9500.0) RadarState(9500.0, 10000.0); RadarState(10000.0, -10000.0) RadarState(10000.0, -9500.0) … RadarState(10000.0, 9500.0) RadarState(10000.0, 10000.0)], statetype = RadarState, discount = 0.95, actions = [(0, 5, 100), (0, 5, 250), (0, 5, 500), (0, 5, 1000), (0, 10, 100), (0, 10, 250), (0, 10, 500), (0, 10, 1000), (0, 15, 100), (0, 15, 250)  …  (355, 35, 500), (355, 35, 1000), (355, 40, 100), (355, 40, 250), (355, 40, 500), (355, 40, 1000), (355, 45, 100), (355, 45, 250), (355, 45, 500), (355, 45, 1000)], obstype = RadarObservation, observation = var\"#27#35\"(), actionindex = Dict((75, 20, 100) => 553, (160, 40, 100) => 1181, (35, 45, 100) => 285, (5, 5, 1000) => 40, (80, 45, 250) => 610, (190, 25, 250) => 1386, (295, 10, 500) => 2131, (210, 15, 500) => 1523, (115, 40, 500) => 859, (155, 15, 1000) => 1128…), transition = var\"#26#34\"(), reward = var\"#28#36\"(), initialstate = ImplicitDistribution{var\"#29#37\", Tuple{}}(var\"#29#37\"(), ())))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dt = 1.0  # sampling interval for radar wave simulation (1 ms step)\n",
    "\n",
    "# Define state space as sampled states for logging\n",
    "states_list = [RadarState(x, y) for x in -10000:500:10000, y in -10000:500:10000]\n",
    "\n",
    "radar = QuickPOMDP(\n",
    "    states = states_list,\n",
    "    statetype = RadarState,\n",
    "    obstype = RadarObservation,\n",
    "\n",
    "    discount = 0.95,\n",
    "\n",
    "    actions = [(steering_angle, beamwidth, power) \n",
    "                for steering_angle in 0:5:355 \n",
    "                for beamwidth in 5:5:45 \n",
    "                for power in [100, 250, 500, 1000]],\n",
    "\n",
    "        transition = function (s, a)\n",
    "            return RadarState(s.x, s.y)\n",
    "        end,\n",
    "\n",
    "    observation = function (a, s)\n",
    "        steer_ang, beamwidth, tx_power = a\n",
    "        println(\"Observation called with state: \", s)\n",
    "\n",
    "        xpos = s.x\n",
    "        ypos = s.y\n",
    "\n",
    "        # clear previous received powers\n",
    "        env.receivers[1].received_power = Float64[]\n",
    "\n",
    "        # create transmitter and add to environment\n",
    "        tx = PointTransmitter(SVector(0.0, 0.0), 1e9, tx_power, 0.0, 0.6, steer_ang, beamwidth, false)\n",
    "        add_transmitter!(env, tx)\n",
    "\n",
    "        println(\"running simulator\")\n",
    "        for t = 1:95\n",
    "            step!(env, dt)\n",
    "        end\n",
    "        rm_transmitter!(env, tx)\n",
    "\n",
    "        powers = env.receivers[1].received_power\n",
    "        noisy_powers = powers .+ abs.(1e-11 .* randn(length(powers)))\n",
    "        return_power, return_index = findmax(noisy_powers)\n",
    "\n",
    "        # convert index → arrival time (seconds)\n",
    "        return_time = return_index * dt\n",
    "\n",
    "        # calculate range (meters)\n",
    "        range = return_time * 300.0  # 300 m/us approximation\n",
    "\n",
    "        println(\"return delay (index): \", return_index)\n",
    "        println(\"return time (s): \", return_time)\n",
    "        println(\"range (m): \", range)\n",
    "\n",
    "        belief_state = boresight_polar_2_cartesian(range, steer_ang + randn() * beamwidth / 2)\n",
    "\n",
    "        return RadarObservation(belief_state[1], belief_state[2])\n",
    "    end,\n",
    "\n",
    "    reward = function (s, a, sp, o)\n",
    "        xpos = s.x\n",
    "        ypos = s.y\n",
    "        steer_ang, beamwidth, tx_power = a\n",
    "\n",
    "        tracking_error = sqrt((xpos - o.x_belief)^2 + (ypos - o.y_belief)^2)\n",
    "\n",
    "        if tracking_error < 10000\n",
    "            println(\"Tracking SUCCESS: error = \", tracking_error)\n",
    "            return 100.0\n",
    "        else\n",
    "            power_cost = tx_power / 100e3\n",
    "            norm_tracking_error = tracking_error / sqrt(2 * (x_max_size^2 + y_max_size^2))\n",
    "            return -100 * (power_cost + norm_tracking_error)\n",
    "        end\n",
    "    end,\n",
    "\n",
    "    initialstate = ImplicitDistribution(rng -> begin\n",
    "        x = (2rand(rng)-1)*10000\n",
    "        y = (2rand(rng)-1)*10000\n",
    "\n",
    "        pos_ref = SVector(x, y)\n",
    "        snapped_pos_ref = snap_to_grid(env.grid_x, pos_ref)\n",
    "        rx_ref = Reflector(snapped_pos_ref, 0.9, 1e10, false)\n",
    "        add_reflector!(env, rx_ref)\n",
    "\n",
    "        return RadarState(x, y)   # ← no comma\n",
    "    end),\n",
    "\n",
    "    isterminal = s -> false\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b137e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discount(radar) = 0.95\n",
      "(actions(radar))[2000] = (275, 25, 1000)\n",
      "rand(initialstate(radar)) = RadarState(5828.257691138699, -3563.866658926456)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RadarState(5828.257691138699, -3563.866658926456)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@show discount(radar)\n",
    "@show actions(radar)[2000]\n",
    "@show rand(initialstate(radar)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "080d308d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation called with state: RadarState(4041.8235659650145, 1352.625206808656)\n",
      "running simulator\n",
      "return delay (index): 7\n",
      "return time (s): 7.0\n",
      "range (m): 2100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-95.41432842461268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#using BenchmarkTools\n",
    "\n",
    "policy = FunctionPolicy(o -> POMDPs.actions(radar)[1])\n",
    "sim = RolloutSimulator(max_steps=1)\n",
    "simulate(sim, radar, policy)\n",
    "#@btime simulate(sim, radar, policy)\n",
    "#bench_result = @benchmark simulate($sim, $radar, $policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62fc11",
   "metadata": {},
   "source": [
    "## 2. Create Updater (Particle Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc32fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Getting action ===\n",
      "Observation called with state: RadarState(-8476.4137400582, -8292.295129672248)\n",
      "running simulator\n",
      "return delay (index): 77\n",
      "return time (s): 77.0\n",
      "range (m): 23100.0\n",
      "FOROLLOUT CALLED\n",
      "Observation called with state: RadarState(-8476.4137400582, -8292.295129672248)\n",
      "running simulator\n",
      "return delay (index): 23\n",
      "return time (s): 23.0\n",
      "range (m): 6900.0\n",
      "  [FORollout step 1] reward = -82.73418675384131\n",
      "  0.607952 seconds (122.71 k allocations: 8.315 MiB, 10.85% compilation time)\n",
      "\n",
      "=== Chosen action ===\n",
      "Action chosen: (250, 30, 250)\n"
     ]
    }
   ],
   "source": [
    "pomdp = radar\n",
    "\n",
    "# Build rollout policy (random greedy)\n",
    "rollout_policy = EpsGreedyPolicy(pomdp, 0.1)\n",
    "\n",
    "function pomcp_solve(m)\n",
    "    solver = POMCPSolver(tree_queries=3,\n",
    "                         c=1.0, max_depth=20,\n",
    "                         default_action=rand(actions(m)),\n",
    "                         estimate_value=estimate_value=FORollout(FunctionPolicy(s -> begin\n",
    "                         println(\"FOROLLOUT CALLED\")\n",
    "                         pos_ref = snap_to_grid(env.grid_x, SVector(s.x, s.y))\n",
    "                         refl = Reflector(pos_ref, 0.9, 1e10, false)\n",
    "                         add_reflector!(env, refl)\n",
    "                     \n",
    "                         total_reward = 0.0\n",
    "                         discount = 1.0\n",
    "                         max_steps = 20  # rollout horizon\n",
    "                         current_state = s\n",
    "                     \n",
    "                         for step in 1:max_steps\n",
    "                             a = rand(actions(m))\n",
    "                             sp = transition(m, current_state, a)\n",
    "                             o = observation(m, a, sp)\n",
    "                             r = reward(m, current_state, a, sp, o)\n",
    "\n",
    "                             println(\"  [FORollout step $(step)] reward = \", r)\n",
    "                     \n",
    "                             total_reward += discount * r\n",
    "                             discount *= m.discount\n",
    "                     \n",
    "                             println(\"  [FORollout step $(step)] reward = \", r, \"  total_reward = \", total_reward)\n",
    "                     \n",
    "                             if r >= 100.0\n",
    "                                 println(\"  [FORollout step $(step)] SUCCESS → terminating rollout.\")\n",
    "                                 #break\n",
    "                             end\n",
    "                     \n",
    "                             current_state = sp\n",
    "                         end\n",
    "                     \n",
    "                         env.reflectors = filter(r -> r != refl, env.reflectors)\n",
    "                     \n",
    "                         return total_reward\n",
    "                     end))\n",
    "                     \n",
    "                         )\n",
    "    return solve(solver, m)\n",
    "end\n",
    "\n",
    "# Solve POMDP\n",
    "env.reflectors = Reflector[]\n",
    "pomcp_p = pomcp_solve(pomdp)\n",
    "\n",
    "rng = MersenneTwister(42)  # or no seed\n",
    "b = ParticleCollection([rand(rng, initialstate(pomdp)) for _ in 1:100])\n",
    "\n",
    "println(\"\\n=== Getting action ===\")\n",
    "@time a = action(pomcp_p, b)\n",
    "\n",
    "println(\"\\n=== Chosen action ===\")\n",
    "println(\"Action chosen: \", a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0ab2219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMCPPlanner{QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}, FunctionPolicy{var\"#83#84\"{QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}}}, BasicPOMCP.var\"#3#7\", TaskLocalRNG}(POMCPSolver\n",
       "  max_depth: Int64 20\n",
       "  c: Float64 1.0\n",
       "  tree_queries: Int64 1000\n",
       "  max_time: Float64 Inf\n",
       "  tree_in_info: Bool false\n",
       "  default_action: Tuple{Int64, Int64, Int64}\n",
       "  rng: TaskLocalRNG TaskLocalRNG()\n",
       "  estimate_value: FunctionPolicy{var\"#83#84\"{QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}}}\n",
       "  time: #3 (function of type BasicPOMCP.var\"#3#7\")\n",
       ", QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}((stateindex = Dict{RadarState, Int64}(RadarState(-4500.0, 2000.0) => 996, RadarState(8000.0, -7000.0) => 283, RadarState(9500.0, 6500.0) => 1393, RadarState(-2000.0, -7000.0) => 263, RadarState(0.0, -6500.0) => 308, RadarState(-3000.0, -2500.0) => 630, RadarState(1000.0, -8500.0) => 146, RadarState(-3500.0, 3500.0) => 1121, RadarState(-500.0, 10000.0) => 1660, RadarState(-4000.0, 1000.0) => 915…), isterminal = var\"#58#66\"(), states = RadarState[RadarState(-10000.0, -10000.0) RadarState(-10000.0, -9500.0) … RadarState(-10000.0, 9500.0) RadarState(-10000.0, 10000.0); RadarState(-9500.0, -10000.0) RadarState(-9500.0, -9500.0) … RadarState(-9500.0, 9500.0) RadarState(-9500.0, 10000.0); … ; RadarState(9500.0, -10000.0) RadarState(9500.0, -9500.0) … RadarState(9500.0, 9500.0) RadarState(9500.0, 10000.0); RadarState(10000.0, -10000.0) RadarState(10000.0, -9500.0) … RadarState(10000.0, 9500.0) RadarState(10000.0, 10000.0)], statetype = RadarState, discount = 0.95, actions = [(0, 5, 100), (0, 5, 250), (0, 5, 500), (0, 5, 1000), (0, 10, 100), (0, 10, 250), (0, 10, 500), (0, 10, 1000), (0, 15, 100), (0, 15, 250)  …  (355, 35, 500), (355, 35, 1000), (355, 40, 100), (355, 40, 250), (355, 40, 500), (355, 40, 1000), (355, 45, 100), (355, 45, 250), (355, 45, 500), (355, 45, 1000)], obstype = RadarObservation, observation = var\"#55#63\"(), actionindex = Dict((75, 20, 100) => 553, (160, 40, 100) => 1181, (35, 45, 100) => 285, (5, 5, 1000) => 40, (80, 45, 250) => 610, (190, 25, 250) => 1386, (295, 10, 500) => 2131, (210, 15, 500) => 1523, (115, 40, 500) => 859, (155, 15, 1000) => 1128…), transition = var\"#54#62\"(), reward = var\"#56#64\"(), initialstate = ImplicitDistribution{var\"#57#65\", Tuple{}}(var\"#57#65\"(), ()))), FunctionPolicy{var\"#83#84\"{QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}}}(var\"#83#84\"{QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}}(QuickPOMDP{Base.UUID(\"91cc972e-1ca2-4a9b-8e3a-bcfadbe222e7\"), RadarState, Tuple{Int64, Int64, Int64}, RadarObservation, @NamedTuple{stateindex::Dict{RadarState, Int64}, isterminal::var\"#58#66\", states::Matrix{RadarState}, statetype::DataType, discount::Float64, actions::Vector{Tuple{Int64, Int64, Int64}}, obstype::DataType, observation::var\"#55#63\", actionindex::Dict{Tuple{Int64, Int64, Int64}, Int64}, transition::var\"#54#62\", reward::var\"#56#64\", initialstate::ImplicitDistribution{var\"#57#65\", Tuple{}}}}((stateindex = Dict{RadarState, Int64}(RadarState(-4500.0, 2000.0) => 996, RadarState(8000.0, -7000.0) => 283, RadarState(9500.0, 6500.0) => 1393, RadarState(-2000.0, -7000.0) => 263, RadarState(0.0, -6500.0) => 308, RadarState(-3000.0, -2500.0) => 630, RadarState(1000.0, -8500.0) => 146, RadarState(-3500.0, 3500.0) => 1121, RadarState(-500.0, 10000.0) => 1660, RadarState(-4000.0, 1000.0) => 915…), isterminal = var\"#58#66\"(), states = RadarState[RadarState(-10000.0, -10000.0) RadarState(-10000.0, -9500.0) … RadarState(-10000.0, 9500.0) RadarState(-10000.0, 10000.0); RadarState(-9500.0, -10000.0) RadarState(-9500.0, -9500.0) … RadarState(-9500.0, 9500.0) RadarState(-9500.0, 10000.0); … ; RadarState(9500.0, -10000.0) RadarState(9500.0, -9500.0) … RadarState(9500.0, 9500.0) RadarState(9500.0, 10000.0); RadarState(10000.0, -10000.0) RadarState(10000.0, -9500.0) … RadarState(10000.0, 9500.0) RadarState(10000.0, 10000.0)], statetype = RadarState, discount = 0.95, actions = [(0, 5, 100), (0, 5, 250), (0, 5, 500), (0, 5, 1000), (0, 10, 100), (0, 10, 250), (0, 10, 500), (0, 10, 1000), (0, 15, 100), (0, 15, 250)  …  (355, 35, 500), (355, 35, 1000), (355, 40, 100), (355, 40, 250), (355, 40, 500), (355, 40, 1000), (355, 45, 100), (355, 45, 250), (355, 45, 500), (355, 45, 1000)], obstype = RadarObservation, observation = var\"#55#63\"(), actionindex = Dict((75, 20, 100) => 553, (160, 40, 100) => 1181, (35, 45, 100) => 285, (5, 5, 1000) => 40, (80, 45, 250) => 610, (190, 25, 250) => 1386, (295, 10, 500) => 2131, (210, 15, 500) => 1523, (115, 40, 500) => 859, (155, 15, 1000) => 1128…), transition = var\"#54#62\"(), reward = var\"#56#64\"(), initialstate = ImplicitDistribution{var\"#57#65\", Tuple{}}(var\"#57#65\"(), ()))))), BasicPOMCP.var\"#3#7\"(), TaskLocalRNG(), Int64[], nothing)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function deep_rollout(m, s, policy, max_steps=20)\n",
    "    total_reward = 0.0\n",
    "    discount = 1.0\n",
    "    current_state = s\n",
    "    print(\"rolloutfn\")\n",
    "    for step in 1:max_steps\n",
    "        a = rand(actions(m))\n",
    "        sp = transition(m, current_state, a)\n",
    "        o = observation(m, a, sp)\n",
    "        r = reward(m, current_state, a, sp, o)\n",
    "\n",
    "        println(\"  [Rollout step $(step)] reward = \", r)\n",
    "\n",
    "        total_reward += discount * r\n",
    "        discount *= m.discount\n",
    "\n",
    "        if r >= 100.0\n",
    "            println(\"  [Rollout step $(step)] SUCCESS → terminating.\")\n",
    "            break\n",
    "        end\n",
    "\n",
    "        current_state = sp\n",
    "    end\n",
    "\n",
    "    env.reflectors = filter(r -> r != refl, env.reflectors)\n",
    "\n",
    "    return total_reward\n",
    "end\n",
    "\n",
    "\n",
    "function pomcp_solve(m)\n",
    "    solver = POMCPSolver(tree_queries=1000,\n",
    "                         c=1.0, max_depth=20,\n",
    "                         default_action=rand(actions(m)),\n",
    "                         estimate_value = FunctionPolicy(s -> deep_rollout(m, s, rollout_policy))\n",
    "                         )\n",
    "    return solve(solver, m)\n",
    "end\n",
    "\n",
    "# Solve POMDP\n",
    "env.reflectors = Reflector[]\n",
    "pomcp_p = pomcp_solve(pomdp)\n",
    "\n",
    "# rng = MersenneTwister(42)  # or no seed\n",
    "# b = ParticleCollection([rand(rng, initialstate(pomdp)) for _ in 1:100])\n",
    "\n",
    "# println(\"\\n=== Getting action ===\")\n",
    "# @time a = action(pomcp_p, b)\n",
    "\n",
    "# println(\"\\n=== Chosen action ===\")\n",
    "# println(\"Action chosen: \", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4717ed4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Manual rollout starting from sampled state ===\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 48\n",
      "return time (s): 48.0\n",
      "range (m): 14400.0\n",
      "  [Manual rollout step 1] reward = -58.92511866478149  total_reward = -58.92511866478149\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 56\n",
      "return time (s): 56.0\n",
      "range (m): 16800.0\n",
      "  [Manual rollout step 2] reward = -131.00447134148683  total_reward = -183.37936643919397\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 79\n",
      "return time (s): 79.0\n",
      "range (m): 23700.0\n",
      "  [Manual rollout step 3] reward = -82.7061706249007  total_reward = -258.02168542816685\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 13\n",
      "return time (s): 13.0\n",
      "range (m): 3900.0\n",
      "  [Manual rollout step 4] reward = -66.28988498372344  total_reward = -314.8569755660867\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 90\n",
      "return time (s): 90.0\n",
      "range (m): 27000.0\n",
      "  [Manual rollout step 5] reward = -178.5316916405407  total_reward = -460.27215423037984\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 74\n",
      "return time (s): 74.0\n",
      "range (m): 22200.0\n",
      "  [Manual rollout step 6] reward = -86.17102123660555  total_reward = -526.9496478281728\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 90\n",
      "return time (s): 90.0\n",
      "range (m): 27000.0\n",
      "  [Manual rollout step 7] reward = -181.32365070771684  total_reward = -660.2391930419356\n",
      "Observation called with state: RadarState(-9240.707757690525, 1695.4713698231183)\n",
      "running simulator\n",
      "return delay (index): 58\n",
      "return time (s): 58.0\n",
      "range (m): 17400.0\n",
      "Tracking SUCCESS: error = 8011.407745099578\n",
      "  [Manual rollout step 8] reward = 100.0  total_reward = -590.4054634325606\n",
      "  [Manual rollout step 8] SUCCESS → terminating rollout.\n",
      "\n",
      "Estimated rollout reward (Q) from sampled state: -590.4054634325606\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# === Manual rollout to inspect Q value ===\n",
    "############################################\n",
    "\n",
    "function run_manual_rollout(m, s::RadarState)\n",
    "    pos_ref = snap_to_grid(env.grid_x, SVector(s.x, s.y))\n",
    "    refl = Reflector(pos_ref, 0.9, 1e10, false)\n",
    "    add_reflector!(env, refl)\n",
    "\n",
    "    total_reward = 0.0\n",
    "    discount_factor = 1.0\n",
    "    max_steps = 20\n",
    "    current_state = s\n",
    "\n",
    "    println(\"\\n=== Manual rollout starting from sampled state ===\")\n",
    "\n",
    "    for step in 1:max_steps\n",
    "        a = rand(actions(m))\n",
    "        sp = transition(m, current_state, a)\n",
    "        o = observation(m, a, sp)\n",
    "        r = reward(m, current_state, a, sp, o)\n",
    "\n",
    "        total_reward += discount_factor * r\n",
    "        discount_factor *= discount(m)  \n",
    "\n",
    "        println(\"  [Manual rollout step $(step)] reward = \", r, \"  total_reward = \", total_reward)\n",
    "\n",
    "        if r >= 100.0\n",
    "            println(\"  [Manual rollout step $(step)] SUCCESS → terminating rollout.\")\n",
    "            break\n",
    "        end\n",
    "\n",
    "        current_state = sp\n",
    "    end\n",
    "\n",
    "    env.reflectors = filter(r -> r != refl, env.reflectors)\n",
    "\n",
    "    return total_reward\n",
    "end\n",
    "\n",
    "# Run manual rollout from sampled state\n",
    "s = rand(b)\n",
    "rollout_reward = run_manual_rollout(pomdp, s)\n",
    "\n",
    "println(\"\\nEstimated rollout reward (Q) from sampled state: \", rollout_reward)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.3",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
